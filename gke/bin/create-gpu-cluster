#!/bin/sh -e

CLUSTER_NAME="krm-examples-trainer-server"
REGION="us-central1"
PROJECT_ID=$(gcloud config get-value project)

# Create the cluster with a default CPU-only node pool
gcloud beta container clusters create "${CLUSTER_NAME}" \
  --project "${PROJECT_ID}" \
  --region "${REGION}" \
  --release-channel "regular" \
  --machine-type "e2-standard-4" \
  --num-nodes "1" \
  --node-locations "${REGION}-a,${REGION}-b" \
  --enable-ip-alias \
  --node-labels=node-type=default \
  --quiet

echo "✅ Default node pool created."

# Add the Inference Node Pool (NVIDIA L4)
gcloud beta container node-pools create "inference-pool" \
  --project "${PROJECT_ID}" \
  --cluster "${CLUSTER_NAME}" \
  --region "${REGION}" \
  --machine-type "g2-standard-4" \
  --accelerator type=nvidia-l4,count=1 \
  --num-nodes "1" \
  --node-labels=node-type=inference \
  --quiet

echo "✅ Inference node pool (L4 GPUs) created."
echo "Installing NVIDIA drivers for inference pool..."
kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml

# Add the Training Node Pool (NVIDIA A100)
gcloud beta container node-pools create "training-pool" \
  --project "${PROJECT_ID}" \
  --cluster "${CLUSTER_NAME}" \
  --region "${REGION}" \
  --machine-type "a2-highgpu-1g" \
  --accelerator type=nvidia-tesla-a100,count=1 \
  --num-nodes "1" \
  --node-labels=node-type=training \
  --quiet

echo "✅ Training node pool (A100 GPUs) created."
echo "You may need to wait a few minutes for the NVIDIA drivers to finish installing on all pools."
echo "Cluster '${CLUSTER_NAME}' is ready."
